# Machine Learning Roadmap

Machine Learning 
├── Data Types 
│ ├── Structured Data 
│ ├── Unstructured Data 
│ └── Semi-Structured Data 
├── Learning Paradigms 
│ 
├── Supervised Learning 
│ ├── Regression 
│ │ ├── Linear Regression 
│ │ ├── Polynomial Regression 
│ │ ├── Support Vector Regression (SVR) 
│ │ ├── Decision Tree Regression 
│ │ ├── Random Forest Regression 
│ │ ├── Neural Networks for Regression 
│ │ ├── Ensemble Methods 
│ │ │ ├── Bagging 
│ │ │ │ └── Random Forests 
│ │ │ └── Boosting 
│ │ │ ├── Gradient Boosting Machines (GBM) 
│ │ │ ├── XGBoost 
│ │ │ ├── LightGBM 
│ │ │ ├── CatBoost 
│ │ │ └── AdaBoost 
│ ├── Classification 
│ │ ├── Logistic Regression 
│ │ ├── Decision Trees 
│ │ │ ├── Bagging 
│ │ │ │ └── Random Forests 
│ │ │ └── Boosting 
│ │ │ ├── Gradient Boosting Machines (GBM) 
│ │ │ ├── XGBoost 
│ │ │ ├── LightGBM 
│ │ │ ├── CatBoost 
│ │ │ └── AdaBoost 
│ │ ├── Support Vector Machines (SVM) 
│ │ ├── K-Nearest Neighbors (KNN) 
│ │ │ └── Distance Metrics 
│ │ │ ├── Euclidean Distance 
│ │ │ ├── Manhattan Distance 
│ │ │ ├── Jaccard Distance 
│ │ │ └── Cosine Similarity 
│ │ ├── Naive Bayes Classifiers 
│ │ │ ├── Gaussian Naive Bayes 
│ │ │ ├── Multinomial Naive Bayes 
│ │ │ └── Bernoulli Naive Bayes 
│ │ ├── Neural Networks 
│ │ │ ├── Feedforward Neural Networks 
│ │ │ ├── Convolutional Neural Networks (CNNs) 
│ │ │ ├── Recurrent Neural Networks (RNNs) 
│ │ │ │ ├── Long Short-Term Memory (LSTM) 
│ │ │ │ └── Gated Recurrent Units (GRU) 
│ │ │ └── Transformers 
│ │ │ ├── BERT (Bidirectional Encoder Representations from Transformers) 
│ │ │ └── GPT Series (Generative Pre-trained Transformers) 
│ │ ├── Discriminant Analysis 
│ │ │ ├── Linear Discriminant Analysis (LDA) 
│ │ │ └── Quadratic Discriminant Analysis (QDA) 
│ │ ├── Ensemble Methods 
│ │ │ ├── Bagging 
│ │ │ │ └── Random Forests 
│ │ │ ├── Boosting 
│ │ │ │ ├── Gradient Boosting Machines (GBM) 
│ │ │ │ ├── XGBoost 
│ │ │ │ ├── LightGBM 
│ │ │ │ ├── CatBoost 
│ │ │ │ └── AdaBoost 
│ │ │ ├── Stacking 
│ │ │ ├── Blending 
│ │ │ └── Voting Classifiers 
│ │ └── Probabilistic Models 
│ │ ├── Hidden Markov Models (HMMs) 
│ │ └── Bayesian Networks 
│ ├── Model Evaluation Metrics 
│ │ ├── Accuracy 
│ │ ├── Precision 
│ │ ├── Recall 
│ │ ├── F1 Score 
│ │ ├── ROC-AUC Curve 
│ │ ├── Log Loss 
│ │ └── Confusion Matrix 
│ 
├── Unsupervised Learning 
│ ├── Clustering Algorithms 
│ │ ├── K-Means Clustering 
│ │ ├── Hierarchical Clustering 
│ │ ├── DBSCAN (Density-Based Spatial Clustering) 
│ │ ├── Gaussian Mixture Models (GMM) 
│ │ └── Self-Organizing Maps (SOMs) 
│ ├── Dimensionality Reduction 
│ │ ├── Principal Component Analysis (PCA) 
│ │ ├── t-Distributed Stochastic Neighbor Embedding (t-SNE) 
│ │ ├── Autoencoders 
│ │ ├── Factor Analysis 
│ │ └── Independent Component Analysis (ICA) 
│ ├── Association Rule Learning 
│ │ ├── Apriori Algorithm 
│ │ ├── Eclat Algorithm 
│ │ └── FP-Growth Algorithm 
│ ├── Anomaly Detection 
│ │ ├── One-Class SVM 
│ │ ├── Isolation Forest 
│ │ └── Local Outlier Factor (LOF) 
│ ├── Topic Modeling 
│ │ ├── Latent Dirichlet Allocation (LDA) 
│ │ └── Latent Semantic Analysis (LSA) 
│ ├── Model Evaluation Metrics 
│ │ ├── Silhouette Score 
│ │ ├── Calinski-Harabasz Index 
│ │ └── Davies-Bouldin Index 
│ 
├── Semi-Supervised Learning 
│ ├── Self-Training Models 
│ ├── Co-Training Models 
│ ├── Graph-Based Models 
│ └── Generative Models for Semi-Supervised Learning 
│ 
├── Reinforcement Learning 
│ ├── Model-Free Methods 
│ │ ├── Q-Learning 
│ │ ├── SARSA (State-Action-Reward-State-Action) 
│ │ └── Deep Q-Networks (DQN) 
│ ├── Policy-Based Methods 
│ │ ├── Policy Gradients 
│ │ └── Actor-Critic Models 
│ ├── Model-Based Methods 
│ │ ├── Dynamic Programming 
│ │ └── Monte Carlo Methods 
│ ├── Hierarchical Reinforcement Learning 
│ └── Multi-Agent Reinforcement Learning 
│ 
├── Deep Learning 
│ ├── Neural Network Architectures 
│ │ ├── Feedforward Neural Networks 
│ │ ├── Convolutional Neural Networks (CNNs) 
│ │ ├── Recurrent Neural Networks (RNNs) 
│ │ │ ├── Long Short-Term Memory (LSTM) 
│ │ │ └── Gated Recurrent Units (GRU) 
│ │ ├── Transformers 
│ │ │ ├── BERT 
│ │ │ └── GPT Series 
│ │ └── Capsule Networks 
│ ├── Generative Models 
│ │ ├── Generative Adversarial Networks (GANs) 
│ │ ├── Variational Autoencoders (VAEs) 
│ │ └── Flow-Based Models 
│ ├── Self-Supervised Learning 
│ ├── Attention Mechanisms 
│ ├── Transfer Learning 
│ │ ├── Pre-Trained Models 
│ │ │ ├── ImageNet Models (ResNet, VGG, Inception) 
│ │ │ └── NLP Models (BERT, GPT) 
│ │ └── Fine-Tuning Techniques 
│ ├── Deep Reinforcement Learning 
│ └── Neural Architecture Search (NAS) 
│ 
├── Meta-Learning 
│ ├── Few-Shot Learning 
│ ├── Zero-Shot Learning 
│ ├── Optimization-Based Meta-Learning 
│ │ └── Model-Agnostic Meta-Learning (MAML) 
│ └── Metric-Based Meta-Learning 
│ ├── Siamese Networks 
│ └── Prototypical Networks 
│ 
├── Ensemble Methods 
│ ├── Bagging 
│ │ ├── Random Forests 
│ │ └── Extra Trees (Extremely Randomized Trees) 
│ ├── Boosting 
│ │ ├── Gradient Boosting Machines (GBM) 
│ │ ├── XGBoost 
│ │ ├── LightGBM 
│ │ ├── CatBoost 
│ │ └── AdaBoost 
│ ├── Stacking 
│ ├── Blending 
│ └── Voting Classifiers 
│ 
├── Optimization Algorithms 
│ ├── Gradient Descent Variants 
│ │ ├── Batch Gradient Descent 
│ │ ├── Stochastic Gradient Descent (SGD) 
│ │ ├── Mini-Batch Gradient Descent 
│ │ └── Momentum-Based Methods 
│ ├── Adaptive Learning Rate Methods 
│ │ ├── AdaGrad 
│ │ ├── RMSProp 
│ │ ├── Adam 
│ │ ├── AdaDelta 
│ │ └── Nadam 
│ ├── Second-Order Methods 
│ │ ├── Newton's Method 
│ │ └── Quasi-Newton Methods (L-BFGS) 
│ └── Evolutionary Algorithms 
│ ├── Genetic Algorithms 
│ ├── Particle Swarm Optimization 
│ └── Simulated Annealing 
│ 
├── Model Evaluation and Validation 
│ ├── Cross-Validation Techniques 
│ │ ├── K-Fold Cross-Validation 
│ │ ├── Stratified K-Fold Cross-Validation 
│ │ ├── Leave-One-Out Cross-Validation (LOOCV) 
│ │ └── Time Series Split 
│ ├── Hyperparameter Tuning 
│ │ ├── Grid Search 
│ │ ├── Random Search 
│ │ ├── Bayesian Optimization 
│ │ ├── Genetic Algorithms 
│ │ └── Hyperband 
│ ├── Regularization Techniques 
│ │ ├── Lasso Regression (L1 Regularization) 
│ │ ├── Ridge Regression (L2 Regularization) 
│ │ ├── Elastic Net 
│ │ ├── Dropout (for Neural Networks) 
│ │ └── Early Stopping 
│ ├── Evaluation Metrics 
│ │ ├── Regression Metrics 
│ │ │ ├── Mean Squared Error (MSE) 
│ │ │ ├── Root Mean Squared Error (RMSE) 
│ │ │ ├── Mean Absolute Error (MAE) 
│ │ │ ├── R-squared (Coefficient of Determination) 
│ │ │ └── Mean Absolute Percentage Error (MAPE) 
│ │ ├── Classification Metrics 
│ │ │ ├── Accuracy │ │ │ ├── Precision 
│ │ │ ├── Recall (Sensitivity) 
│ │ │ ├── Specificity 
│ │ │ ├── F1 Score 
│ │ │ ├── ROC-AUC Curve 
│ │ │ ├── Log Loss 
│ │ │ └── Confusion Matrix 
│ │ └── Clustering Metrics 
│ │ ├── Silhouette Score 
│ │ ├── Calinski-Harabasz Index 
│ │ └── Davies-Bouldin Index 
│ 
├── Data Preprocessing and Feature Engineering 
│ ├── Data Cleaning 
│ │ ├── Handling Missing Values 
│ │ └── Outlier Detection and Removal 
│ ├── Feature Scaling 
│ │ ├── Normalization 
│ │ └── Standardization 
│ ├── Encoding Categorical Variables 
│ │ ├── One-Hot Encoding 
│ │ ├── Label Encoding 
│ │ └── Target Encoding 
│ ├── Feature Selection 
│ │ ├── Filter Methods 
│ │ ├── Wrapper Methods 
│ │ └── Embedded Methods 
│ ├── Feature Extraction 
│ │ ├── Principal Component Analysis (PCA) 
│ │ ├── Singular Value Decomposition (SVD) 
│ │ └── Independent Component Analysis (ICA) 
│ ├── Imbalanced Data Techniques 
│ │ ├── Oversampling 
│ │ │ └── SMOTE (Synthetic Minority Over-sampling Technique) 
│ │ ├── Undersampling 
│ │ └── Ensemble Techniques 
│ └── Automated Feature Engineering 
│ └── Feature Tools 
│ 
├── Specialized Domains and Applications 
│ ├── Natural Language Processing (NLP) 
│ │ ├── Text Preprocessing 
│ │ │ ├── Tokenization 
│ │ │ ├── Stemming 
│ │ │ └── Lemmatization 
│ │ ├── Sentiment Analysis 
│ │ ├── Named Entity Recognition (NER) 
│ │ ├── Machine Translation 
│ │ ├── Language Modeling 
│ │ └── Topic Modeling 
│ ├── Computer Vision 
│ │ ├── Image Classification 
│ │ ├── Object Detection 
│ │ │ ├── YOLO (You Only Look Once) 
│ │ │ ├── Faster R-CNN 
│ │ │ └── SSD (Single Shot MultiBox Detector) 
│ │ ├── Image Segmentation 
│ │ │ ├── Semantic Segmentation 
│ │ │ └── Instance Segmentation 
│ │ ├── Facial Recognition 
│ │ └── Style Transfer 
│ ├── Time Series Analysis 
│ │ ├── ARIMA Models 
│ │ ├── Exponential Smoothing 
│ │ ├── Prophet Model 
│ │ ├── LSTMs and GRUs 
│ │ └── Seasonal Decomposition 
│ ├── Recommender Systems 
│ │ ├── Collaborative Filtering 
│ │ │ ├── User-Based 
│ │ │ └── Item-Based 
│ │ ├── Content-Based Filtering 
│ │ └── Hybrid Models 
│ ├── Graph Machine Learning 
│ │ ├── Graph Neural Networks (GNNs) 
│ │ ├── Node Classification 
│ │ ├── Link Prediction 
│ │ └── Graph Embeddings 
│ └── Speech Recognition and Audio Processing 
│ ├── Automatic Speech Recognition (ASR) 
│ ├── Speech Synthesis (Text-to-Speech) 
│ └── Audio Classification 
│ 
├── Ethics, Fairness, and Interpretability 
│ ├── Bias Mitigation Techniques 
│ │ ├── Pre-Processing Methods 
│ │ ├── In-Processing Methods 
│ │ └── Post-Processing Methods 
│ ├── Fairness Metrics 
│ │ ├── Demographic Parity 
│ │ ├── Equal Opportunity 
│ │ └── Equalized Odds 
│ ├── Explainable AI (XAI) 
│ │ ├── SHAP (SHapley Additive exPlanations) 
│ │ ├── LIME (Local Interpretable Model-agnostic Explanations) 
│ │ ├── Saliency Maps 
│ │ └── Partial Dependence Plots 
│ ├── Privacy-Preserving Techniques 
│ │ ├── Differential Privacy 
│ │ ├── Federated Learning 
│ │ └── Homomorphic Encryption 
│ └── Ethical Considerations 
│ ├── AI Governance 
│ ├── Ethical Frameworks 
│ └── Responsible AI Practices 
│ 
├── Infrastructure, Deployment, and MLOps 
│ ├── Model Deployment 
│ │ ├── RESTful APIs 
│ │ ├── Microservices Architecture 
│ │ └── Model Serialization (Pickle, Joblib) 
│ ├── Scaling and Performance 
│ │ ├── Distributed Computing (Hadoop, Spark) 
│ │ ├── Parallel Processing 
│ │ └── GPU Acceleration 
│ ├── MLOps (Machine Learning Operations) 
│ │ ├── Continuous Integration/Continuous Deployment (CI/CD) 
│ │ ├── Model Monitoring and Logging 
│ │ ├── Version Control for Models and Data (DVC) 
│ │ └── Automated Retraining Pipelines 
│ └── Cloud Services 
│ ├── AWS SageMaker 
│ ├── Google Cloud AI Platform 
│ └── Azure Machine Learning 
│ 
└── Advanced Topics 
├── Probabilistic Models 
│ ├── Bayesian Inference 
│ ├── Markov Chain Monte Carlo (MCMC) 
│ └── Bayesian Networks 
├── Quantum Machine Learning 
├── Federated Learning 
├── Multi-Task Learning 
├── Self-Supervised Learning 
├── Contrastive Learning 
├── Causal Inference 
├── Active Learning 
│ ├── Uncertainty Sampling 
│ ├── Query by Committee 
│ └── Expected Model Change 
├── Transfer Learning 
└── AutoML (Automated Machine Learning) 
├── Hyperparameter Optimization 
└── Neural Architecture Search (NAS)